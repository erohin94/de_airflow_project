## Описание

Задача пайплайна — скачать сырые данные с сайта, трансформировать их в колоночный формат Parquet и загрузить в S3. 

В качестве хранилища буду использовать сервис Minio полностью совместимый с S3 файловый сервер, поэтому Apache Airflow будет работать с ним как с AWS S3 
(Это популярный сервис для хранения файлов от компании Amazon, также S3 можно рассматривать как распределенную файловую систему.).

## Структура пайплайна

![image](https://github.com/user-attachments/assets/8e1c834d-9f57-470b-b5ac-bce209487066)

Пайплайн (DAG) будет состоять из следующих операторов:

`SimpleHttpOperator` - для проверки существования файла на сервере перед его загрузкой

`2 PythonOperator`:

`download_file` — загрузка файла с сайта и перекладывание на S3 в сжатом виде (gzip)

`convert_file` — оператор скачивает файл, загруженный предыдущим оператором, и трансформирует его в формат Parquet, сохраняя результат в S3.

Выполнение последующего шага зависит от успешности предыдущего. Если необходимого файла на сервере нет (check_file), то выполнение загрузки нецелесообразно (download_file), за этим строго следит Airflow.

## Формат

Буду использовать parquet и сжатие gzip при загрузке файла из API.

Parquet выглядит как небольшая файловая система, где значения каждой колонки лежат в отдельных файлах, а также присутствует дополнительный файл с метаданными, где хранится информация о типах колонок и их расположении. То есть чтобы получить значения заданных колонок нужно прочитать только файлы, содержащие данные этих колонок (а не всё целиком).

### Настройка подключений

В UI Airflow Connections прописать следующие параметры

```
{
  "endpoint_url": "http://host.docker.internal:9000",
  "region_name": "us-east-1"
}
```

`AWS Access Key ID` и `AWS Secret Access Key` создаю в Minio. На боковой панели выбрать `Access Keys` далее `Create access keys`.

![image](https://github.com/user-attachments/assets/2a3f9135-71ac-414b-afc8-2b9d460248f7)

Настройки в самом UI Airflow

![image](https://github.com/user-attachments/assets/bde10bf3-b2e6-4017-a3eb-014b40f4af47)

Подключение к API

Прописать HOST `https://eonet.gsfc.nasa.gov/api/v3/events/?` остальную часть ссылки буду предавать в виде `endpoint` в коде.

![image](https://github.com/user-attachments/assets/c89c9dc4-7569-4524-a4ba-4399541fbbd4)

## Про операторы и функции _init_.py и functions.py

**download_file**

Оператор — PythonOperator с `task_id` `download_file`. Его задача загрузить файл с веб-сайта и уложить в S3 в сжатом виде gzip.

PythonOperator принимает объект типа callable, который будет выполнен, поэтому загрузку файла вынес в отдельную функцию `download_dataset`. 

Вынесли эту логику в отдельный модуль или даже пакет (если одного модуля недостаточно), поэтому в репозитории есть модуль functions.py.

Функция `download_dataset` принимает 1 аргумент. Скачивание файла производится пакетом requests. Для работы с S3 использую готовый Hook из Airflow — S3Hook.

Хуки это внешние интерфейсы для работы с различными сервисами: базы данных, внешние API ресурсы, распределенные хранилища типа S3, redis, memcached и т.д. Хуки являются строительными блоками операторов и берут на себя всю логику по взаимодействию с хранилищем конфигов и доступов. Используя хуки можно забыть про головную боль с хранением секретной информации в коде (например, паролей).

S3Hook даёт разработчику высокоуровневый интерфейс для работы с AWS S3, а под капотом использует библиотеку boto3. Самый главный аргумент, который нужно передать S3Hook — aws_conn_id. Соединение также необходимо создать через раздел Admin → Connections.

В коде прописано название бакета S3 объекта, куда будет сохранён загружаемый файл с данными. Далее с веб-ресурса файл сохраняется во временный файл и загружается на S3. Функция возвращает путь до S3 объекта, которым воспользуется следующий оператор.

Чтобы сохранить данные сначала во временный файл, а потом уже загружать в S3 (MinIO), можно использовать NamedTemporaryFile из модуля tempfile.

**convert_file**

Последний оператор задача которого — конвертировать в Parquet и уложить обратно в S3. Код функции преобразования:

Таск `convert_file` принимает аргумент `file_path`. Это путь до S3-объекта, который был сохранён предыдущим таском `download_file`.

Чтобы передать результат выполнения одного оператора в следующий, необходимо ниже написать следующий код:

```
file_path = download_file()
parquet_path = convert_file(file_path)

check_file >> file_path >> parquet_path
```
Он совпадает с тем как обычно вызываются функции и результат передаётся дальше. 
Вся магия "бесшовной" передачи данных от одного оператора к другому происходит за счёт декоратора `@task`. 
При вызове функции, обернутой в `@task`, возвращается инстанс класса `XComArg`. 
Если функция, переданная в `PythonOperator`, возвращает значение, то по умолчанию оно записывается в `XCom`. 
`XComArg` это класс-обёртка или адаптер, который умеет извлекать это значение, т.к. принимает на вход оператор и контекст (в методе resolve). 
Поэтому когда в коде передаём в функцию `convert_file` инстанс класса `XComArg` (`file_path`), Airflow понимает что это за класс и за кулисами получает из XCom результат выполнения `download_file`(путь до загруженного S3-объекта). До появления TaskFlow API это необходимо было делать вручную.

Важно не забывать вызывать функции, обернутые в task, чтобы получить инстанс класса XComArg. check_file в этом случае уже является инстансом оператора SimpleHttpOperator.

Чтобы Airflow Scheduler корректно обнаружил DAG, необходимо также вызвать обернутую в декоратор dag функцию и присвоить значение переменной в глобальном пространстве.

## Результат

![image](https://github.com/user-attachments/assets/978747cf-6030-4a18-b1e5-da0e8c0abf42)

