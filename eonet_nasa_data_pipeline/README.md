## Описание

Задача пайплайна — скачать сырые данные с сайта, трансформировать их в колоночный формат Parquet и загрузить в S3. 

В качестве хранилища буду использовать сервис Minio полностью совместимый с S3 файловый сервер, поэтому Apache Airflow будет работать с ним как с AWS S3 
(Это популярный сервис для хранения файлов от компании Amazon, также S3 можно рассматривать как распределенную файловую систему.).

## Структура пайплайна

![image](https://github.com/user-attachments/assets/8e1c834d-9f57-470b-b5ac-bce209487066)

Пайплайн (DAG) будет состоять из следующих операторов:

`SimpleHttpOperator` - для проверки существования файла на сервере перед его загрузкой

`2 PythonOperator`:

`download_file` — загрузка файла с сайта и перекладывание на S3 в сжатом виде (gzip)

`convert_file` — оператор скачивает файл, загруженный предыдущим оператором, и трансформирует его в формат Parquet, сохраняя результат в S3.

Выполнение последующего шага зависит от успешности предыдущего. Если необходимого файла на сервере нет (check_file), то выполнение загрузки нецелесообразно (download_file), за этим строго следит Airflow.

## Формат

Буду использовать parquet и сжатием gzip при загрузке файла из API.

Parquet выглядит как небольшая файловая система, где значения каждой колонки лежат в отдельных файлах, а также присутствует дополнительный файл с метаданными, где хранится информация о типах колонок и их расположении. То есть чтобы получить значения заданных колонок нужно прочитать только файлы, содержащие данные этих колонок (а не всё целиком).

### Настройка подключений

В UI Airflow Connections прописать следующие параметры

```
{
  "endpoint_url": "http://host.docker.internal:9000",
  "region_name": "us-east-1"
}
```

`AWS Access Key ID` и `AWS Secret Access Key` создаю в Minio. На боковой панели выбрать `Access Keys` далее `Create access keys`.

![image](https://github.com/user-attachments/assets/2a3f9135-71ac-414b-afc8-2b9d460248f7)

Настройки в самом UI Airflow

![image](https://github.com/user-attachments/assets/bde10bf3-b2e6-4017-a3eb-014b40f4af47)

Подключение к API

Прописать HOST `https://eonet.gsfc.nasa.gov/api/v3/events/?` остальную часть ссылки буду предавать в виде `endpoint` в коде.

![image](https://github.com/user-attachments/assets/c89c9dc4-7569-4524-a4ba-4399541fbbd4)

## Про операторы и функции _init_.py и functions.py

Оператор — PythonOperator с `task_id` `download_file`. Его задача загрузить файл с веб-сайта и уложить в S3 в сжатом виде (будем использовать gzip).

PythonOperator принимает объект типа callable, который будет выполнен, поэтому загрузку файла вынес в отдельную функцию `download_dataset`. 

Задача этого модуля загрузить файл с веб-сайта и уложить в S3 в сжатом виде gzip.

Вынесли эту логику в отдельный модуль или даже пакет (если одного модуля недостаточно), поэтому в репозитории есть модуль functions.py.

Функция download_dataset принимает 1 аргумент. Скачивание файла производится пакетом requests. Для работы с S3 использую готовый Hook из Airflow — S3Hook.

Хуки это внешние интерфейсы для работы с различными сервисами: базы данных, внешние API ресурсы, распределенные хранилища типа S3, redis, memcached и т.д. Хуки являются строительными блоками операторов и берут на себя всю логику по взаимодействию с хранилищем конфигов и доступов. Используя хуки можно забыть про головную боль с хранением секретной информации в коде (например, паролей).

S3Hook даёт разработчику высокоуровневый интерфейс для работы с AWS S3, а под капотом использует библиотеку boto3. Самый главный аргумент, который нужно передать S3Hook — aws_conn_id. Соединение также необходимо создать через раздел Admin → Connections.

В коде прописано название бакета S3 объекта, куда будет сохранён загружаемый файл с данными. Далее с веб-ресурса файл сохраняется во временный файл и загружается на S3 с использованием gzip=True. Функция возвращает путь до S3 объекта, которым воспользуется следующий оператор.

Чтобы сохранить данные сначала во временный файл, а потом уже загружать в S3 (MinIO), можно использовать NamedTemporaryFile из модуля tempfile.
